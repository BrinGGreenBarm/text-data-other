여기 짧은 영화 대본이 하나 있습니다
사람과 인공지능이 대화를 나누는
장면인데 아쉽게도 절반이 찢어져
사람이 질문하는 대사만 남아 있고
AI가 뭐라 대답했는지 알 수 없는
상태죠 그런데 저에게 그 어떤
문장이든 입력으로 넣으면 그다음에 올
단어를 가장 그럴 듯하게 예측해 주는
마법 같은 기계가 하나 있습니다.이
기계만 있다면 대본 절반을 기계에
넣어 이어질 첫 번째 단어를 예측하고
그 단어를 다시 대본에 덧붙인 뒤
단어을 예측하고 반복하면 대본을
완성할 수 있겠죠 실제로 채지와의
대화가 이런 식으로
이루어집니다 대규모 언어 모델 럴즈
랭귀지 모델은 어떤 텍스트가 주어졌을
때 다음에 올 단어를 예측하는 매우
정교한 수학적 함수입니다 좀 더
정확하게는 딱 하나의 단어를
확정적으로 예측하는 듯인 다음에 올
단어들에 대한 확률을 구하는 함수죠
실제로 채치T 같은 모델을 만들 때
가상의 사용자와 AI 어시스턴트
사이의 대화를 텍스트로 만들고 여기에
사용자의 질문을 덧붙인 다음 그
질문에 AI가 어떤 식으로 반응할지를
한 단어씩 예측하도록 합니다 이때
가장 확률이 높은 단어만 고르는 것
대신에 가끔씩은 확률이 조금 낮은
단어도 랜덤하게 선택하도록 하면 좀
더 자연스럽고 사람이 풍기는 답변이
되죠 사실 언어 모델 자체는
디터미니스틱 즉 결과가 정해진
결정론적인 모델이지만 이렇게 하면
같은 입력이라도 매번 다른 답변이
나올 수 있는 겁니다 언어 모델은
인터넷에서 수집한 엄청난 양의 텍스트
데이터로 학습되었습니다 gpt3가
학습한 텍스트양을 사람이 직접
읽는다면 하루 24시간 쉬지 않고
읽었을 때 2,600년 이상이 걸릴
겁니다 요즘 나온 수많은 모델들은
훨씬 더 많은 양의 데이터로
훈련되었죠
이러한 모델 훈련은 어떻게 하는
걸까요 모델 속에 수많은 다이어를
조정해 간다고 생각할 수 있는데요
언어 모델의 결과가 바로이 수많은
다이얼들 즉 파라미터 또는 웨이트
가중치라고 부르는 값들에 의해
결정됩니다.이 파라미터들을 바꾸면
모델이 다음 단어를 어떻게 예측할지도
달라지게 되는 거죠
랭귀지 모델 즉 대규모 언어 모델이라
불리는 이유는 이런 파라미터가 수백억
개에서 수천억 개까지 이르기
때문입니다 이렇게 많은 파라미터
값들을 사람이 직접 정할 순 없겠죠
처음엔 랜덤하게 설정된이 파라미터
값들이 훈련을 반복하면 반복할수록
점점 더 그럴듯한 예측을 할 수 있는
값들로 조정되는 겁니다 훈련할 땐
짧게는 몇 단어 길게는 수천 단어로
이루어진 데이터를 사용합니다
기본적으로 훈련은 어떤 텍스트에서
마지막 단어를 뺀 나머지를 모델의
입력으로 넣고 모델이 마지막 단어를
어떻게 예측하는지 확인합니다 그리고
그 예측이 정답에 가까워지도록 모델의
파라미터를 살짝 조정하는 거죠 이때
백프로파게이션 역전파라고도 불리는
알고리즘을 사용해서 점점 더 정답이
가까워지도록 합니다.이 과정을 수없이
많이 반복하게 되면 모델이 학습
데이터에서만 다음 단어를 잘
예측하는게 아니라 처음 보는 문장에
대해서도 그럴듯한 예측을 할 수 있게
되는
겁니다 이렇게 엄청난 수의 파라미터와
수많은 데이터를 처리하기 위해서는
상상을 초월하는 연산량이
필요합니다 얼마나 필요할까요 1초에
10억 번의 덧셈 곧샘 같은 연산할
수 있는 기계가 있다고 해보죠.이
기계로 언어 모델을 훈련시키는데 과연
얼마나
걸릴까요
1년 1만 년
정답은 그것보다 훨씬 더
많은 1억년이 걸립니다 사실 1억년이
걸려도 채치피의 절반도 못
만들 모든 것은 사전 훈련 즉
프리트레이닝 과정입니다 텍스트의 다음
단어를 예측하는 건 좋은 AI
어시스턴트를 만드는 것과는 조금
다르죠 그래서 이렇게 훈련된 모델을
가지고 강화 학습 중에 하나인
RLHF로 더 훈련합니다 사람이
모델의 잘못된 응답을 직접 수정하거나
더 나은 응답을 골라주는 방법 등으로
추가 학습이 이뤄집니다 사용자들이 더
선호하는 방향으로 다음 단어를
예측하도록 조정되는 거죠 프레이트닝
같은 훈련에 필요한 엄청난 연산을
처리하기 위해서는 병렬 처리와 특화된
특수한 컴퓨터 칩 즉 GPU가
필수적으로
필요합니다 하지만 모든 언어 모델의
알고리즘이 병렬 처리에 잘 맞았던 건
아닙니다 2017년 이전까지만 해도
대부분의 모델은 단어를 하나씩
순차적으로 처리했었습니다 하지만
구글의 한 연구팀이 트랜스포머라는
새로운 모델을 발표한 뒤 많은게
바뀌었습니다 트랜스포머는 텍스트를
처음부터 끝까지 순차적으로 읽는 대신
전체 문장을 한꺼번에 병렬로 처리하죠
여기서이 문장들의 각 단어들은 언어
모델이 이해할 수 있는 숫자 벡터들로
바뀌어져서 들어갑니다 ai 훈련은
연속적인 수치 데이터에서 이루어질 수
있기 때문에 언어를 숫자로 인코딩하는
겁니다 각각의 숫자 벡터는 해당
단어의 의미나 맥락을 담고
있습니다 트랜스포머는 어텐션이라는
아주 강력한 연산 알고리즘을 가지고이
숫자 벡터들을 처리합니다 어텐션은이
숫자 벡터들이 서로 정보를
주고받으면서 주변 맥락에 따라서 각
단어의 의미를 적절히 조정할 수 있게
해 줍니다 예를 들어 눈이라는 단어는
주변에 내린다라는 단어가 있으면
하늘에서 내리는 눈을 의미하는 벡터가
되는 겁니다 반대로 보는 눈이 많다는
건 사람 눈을 의미하게 되겠죠
피드포워드 네트워크라는 연산도
트랜스포머 안에 들어가 있습니다
피드포워드는 모델이 더 많은 언어
패턴을 저장할 수 있도록 해
줍니다 이렇게 어텐션과 피드포워드
연산을 여러 층 즉 여러 레이어를
걸쳐서 반복하면 각 단어 벡터는 점점
더 맥락을 잘 반영하게 됩니다 다음
단어를 정확하게 예측하는데 필요한
정보를 압축하고 압축하여 끝까지
전달하죠 그렇게 전달된 마지막
단계에서는이 전체 문맥을 반영한
벡터를 가지고 다음에 올 단어의 확률
분포를 예측하게 됩니다 다음에 올 수
있는 모든 단어들에 대해서 모델이 그
확률을 예측한다고 볼 수 있습니다
이때 앞뒤 단어들에 따라서 그 문맥이
달라지고 이게 반영돼서 예측하는 거죠
이런 모델의 구조는 연구자들이
설계했지만 실제로 어떤 출력을
내는지는 훈련을 통해서 자동으로
조정된 수십억의 파라미터에 결정되는
겁니다 그렇기 때문에이 모델이 다음에
올 단어로 왜이 단어를 예측했는지
설명하는 건 굉장히
어렵습니다 설명하긴 어렵지만 이렇게
만들어진 언어 모델이 생성해내는
텍스트는 굉장히 자연스럽고 신기하고
게다가 유용하죠 저는 맨날 씁니다
